<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Batch Normlization 、Layer Normlization、Group Normlization | Autumn-Cat</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="目标检测的任务（Problem Settings）目标检测的有两个任务：  检测出该图片中物体的类别 检测出对应物体的位置（用一个框表示，即左上坐标和右下坐标）  为了达成我们的目标我们需要一些度量（metric）来作为衡量标准，在目标检测一般使用如下度量：  IoU ：IoU是计算预测位置和目标位置的相差程度的一种度量，其优点是对大框和小框都有很好的适应性。它的计算公式如下：$$IoU = \f">
<meta property="og:type" content="article">
<meta property="og:title" content="Batch Normlization 、Layer Normlization、Group Normlization">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;index.html">
<meta property="og:site_name" content="Autumn-Cat">
<meta property="og:description" content="目标检测的任务（Problem Settings）目标检测的有两个任务：  检测出该图片中物体的类别 检测出对应物体的位置（用一个框表示，即左上坐标和右下坐标）  为了达成我们的目标我们需要一些度量（metric）来作为衡量标准，在目标检测一般使用如下度量：  IoU ：IoU是计算预测位置和目标位置的相差程度的一种度量，其优点是对大框和小框都有很好的适应性。它的计算公式如下：$$IoU = \f">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567155538247.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567241120527.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567323883396.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567324588539.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567501439402.png">
<meta property="og:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567499632449.png">
<meta property="og:updated_time" content="2019-10-22T08:59:54.386Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;09&#x2F;30&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0&#x2F;%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567155538247.png">
  
    <link rel="alternate" href="/atom.xml" title="Autumn-Cat" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Autumn-Cat</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-目标检测简单综述" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/30/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/" class="article-date">
  <time datetime="2018-09-30T03:28:43.000Z" itemprop="datePublished">2018-09-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Batch Normlization 、Layer Normlization、Group Normlization
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="目标检测的任务（Problem-Settings）"><a href="#目标检测的任务（Problem-Settings）" class="headerlink" title="目标检测的任务（Problem Settings）"></a>目标检测的任务（Problem Settings）</h2><p>目标检测的有两个任务：</p>
<ul>
<li>检测出该图片中物体的类别</li>
<li>检测出对应物体的位置（用一个框表示，即左上坐标和右下坐标）</li>
</ul>
<p>为了达成我们的目标我们需要一些度量（metric）来作为衡量标准，在目标检测一般使用如下度量：</p>
<ol>
<li><p><strong>IoU</strong> ：IoU是计算预测位置和目标位置的相差程度的一种度量，其优点是对大框和小框都有很好的适应性。它的计算公式如下：<br>$$<br>IoU = \frac{Area(box_{pred}\cap box_{gt})}{Area(box_{pred}\cup box_{gt})}<br>$$<br>确定好其计算的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">pred:(x1,x2),(x3,x4)</span></span><br><span class="line"><span class="string">gt:(y1,y2),(y3,y4)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">#求出交叉区域面积</span></span><br><span class="line">in_1 = max(x1,y1)</span><br><span class="line">in_2 = max(x2,y2)</span><br><span class="line">in_3 = min(x3,y3)</span><br><span class="line">in_4 = min(x4,y4)</span><br><span class="line">inArea = getArea([in_1,in_2],[in_3,in_4])</span><br><span class="line"><span class="comment">#求出并区域面积</span></span><br><span class="line">area1 = getArea([x1,x2],[x3,x4])</span><br><span class="line">area2 = getArea([y1,y2],[y3,y4])</span><br><span class="line">outArea = area1+area2-inArea</span><br><span class="line"><span class="comment">#计算IoU</span></span><br><span class="line">IoU = outArea/inArea</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>cross entropy</strong>：交叉熵是分类任务的常见损失函数</p>
</li>
<li><p><strong>mAP、AP</strong>：</p>
<p>这一切要从precious和recall开始，其定义如下图所示：</p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567155538247.png" alt="1567155538247"></p>
<p>简而言之，precision是针对预测的（猜的准确率如何）；recall是针对样本的（召回了多少正样本）。</p>
<p>我们需要综合precision和recall，而这就得到了AP。在目标检测时，我们会根据confidence score将检测 n个结果 排序（从大到小）。然后将这n个结果逐个加入集合中得到n个precision和recall（即只有第一个结果的，有第一个第二个，有第一个第二个第三个……）。然后我们将precision组成纵坐标recall组成横坐标得到一个曲线，precision是从 1 逐步下降，recall是从 0 逐步上升，所以我们得到了一个下降的曲线（PR curves）。而PR曲线围成的面积即为AP。</p>
<p>而针对计算面积的方式也有不同的方法，比较常用的是PASCAL和COCO的：</p>
<ul>
<li><p>PASCAL：2008年规则对precision进行平滑，平滑方法是对每一给precision，使用其右边最大的值，在实际使用中，常使用11点进行内插。后来2012改进了，直接求斜线面积（无数个点内插）</p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567241120527.png" alt="1567241120527"></p>
</li>
<li><p>COCO：COCO的mAP就是AP，每一对其进行区分，它使用101个点进行内插 mAP。</p>
</li>
</ul>
</li>
</ol>
<ol start="4">
<li><strong>FPS</strong>：FPS对于一个实时的目标检测模型是非常重要的（在处理视频时）</li>
</ol>
<h2 id="模型思想"><a href="#模型思想" class="headerlink" title="模型思想"></a>模型思想</h2><p>神经网络虽然在分类任务表现出色，但是如果要移植到检测任务上，需要解决三个问题：</p>
<ol>
<li>图片中有些地方有物体，有些地方没有物体。如何确定物体的位置</li>
<li>图片中有多个物体，如何对多个物体进行检测</li>
<li>图片中有些物体大，有些物体小，如何检测出这些多尺度的物体</li>
</ol>
<p>针对如上问题，主要有两种解决思路，一种是one-stage的，还有一种是two-stage</p>
<h3 id="two-stage"><a href="#two-stage" class="headerlink" title="two-stage"></a>two-stage</h3><p>two-stage的思想是将检测分为两步：</p>
<ul>
<li>确定图片中有物体的部分</li>
<li>将该部分送入神经网络中确定类别以及位置</li>
</ul>
<p>一开始的神经网络是暴力的，它就是用不同大小的框滑动选取图片的部分，然后再去做一个物体的分类和定位。然而这种思想太过简略，太没有利用到图片的信息来进行选框。</p>
<p><strong>R-CNN</strong></p>
<p>R-CNN提出可以利用图片中的纹理、边缘、颜色等信息来粗粒度的选取物体可能出现的位置，即候选区域（Region Proposal）。</p>
<p>然而R-CNN有如下问题：</p>
<ul>
<li>将每一个区域都作为CNN的输入，时间开销很大</li>
<li>由于CNN的输入是固定的，而候选区域的形状大小是不确定的。所以需要对候选区域进行形变，但这会造成失真。</li>
</ul>
<p><strong>SPP-net</strong></p>
<p>SPP-net解决了R-CNN的缺点。它首先将一整张图片输入CNN，提取得到特征图。然后将对应候选区域的特征图裁剪出来，这大大减小了时间开销。</p>
<p>然而特征图之后的全连接网络的输入要求是一个固定的大小，所以SPP提出了 空间金字塔池化（Spatial Pyramid Pooling，SPP池化）。</p>
<p>SPP池化的核心是改变的池化的大小（池化一般是$2<em>2$的格子中进行池化）。它将特征图分为$4</em>4$、$2<em>2$、$1</em>1$个格子，分别在这些格子中进行池化。得到一个固定的$16+4+1$的向量。</p>
<p>SPP-net的过程是：单张图片+一次CNN+多个proposal region特征图+多个pooling+多次分类回归。它将多个proposal region特征图存放到磁盘中，然后再分布进行分类回归。这一过程有两个缺点：</p>
<ul>
<li>磁盘的多次读写的时间开销大</li>
<li>这一过程不是端到端的。所以再训练的时候，需要训练好前面的特征提取网络，再固定参数，训练之后的全连接网络。</li>
</ul>
<p><strong>Fast-RCNN</strong></p>
<p>Fast-RCNN修正了SPP-net。它最大的改进是不再将多个proposal region特征图 进行多次回归了。它在将多个特征图进行ROI pooling之后拼接为一个向量。然后直接做分类回归。</p>
<p>ROI pooling是SPP的一个简化版，即它不再将特征图划分为$4<em>4,2</em>2,1<em>1$，而是直接划分为$n</em>n$份。</p>
<p>ROI pooling有一个小缺点是有时候region proposal特征图不一定能够被n整除。所以后来的mask RCNN提出ROI align，当不能整除时，用双线性插值的方式求出浮点数坐标的特征值。</p>
<p><strong>Faster-RCNN</strong></p>
<p>Faster-RCNN是two-stage算法的巅峰之作。它最大的贡献是提出了RPN（region proposal net）。使得选取ROI变成可以学习的。Faster-RCNN的大致流程如下：</p>
<ol>
<li>back bone提取特征图</li>
<li>RPN以特征图作为输入，得到ROI的坐标与第一次类别</li>
<li>结合特征图以及ROI 坐标 得到region proposal特征图</li>
<li>将region proposal特征图送入全连接网络做回归和分类</li>
</ol>
<p><em>附：Faster-RCNN提出了anchor box，然而这个已经不局限于one-stage和two-stage了，所以我将其放在了优化思路中</em></p>
<h3 id="one-stage"><a href="#one-stage" class="headerlink" title="one-stage"></a>one-stage</h3><p>目标的检测的源头都是滑动窗口（sliding windows）。two stage的演化思路方法是改进proposal region的选取方法以及使得将two stage尽量放在一个网络中（网络中的两个部分）。</p>
<p>one-stage的想法更加激进，它认为卷积的操作实际就 可以对应了滑动窗口的滑动部分，卷积得到的特征图中的特征点，就是对应感受野提取出的结果，这里感受野可以类似于滑动剪切的窗口。</p>
<p>YOLO v1是one-stage的开篇之作，他将图片分成$7<em>7$个格子。图片中的物体落在哪个格子中，哪个格子就负责预测该物体。最后输出的特征图是$7*7</em>(C+[confidence,location])$，相当于每个格子预测1个物体（实际上YOLO设计的要更加巧妙，每个格子预测2个物体，通过confidence来共享C），C为类别概率，location为位置信息。这里需要提醒的是，物体的大小可以远大于该格子，因为通过不断的卷积，最后提取的信息远大于那个格子。在损失函数的设计上，YOLO通过将 正样本损失、负样本损失、分类损失、回归损失 都乘以不同权重来调整样本不平衡问题。（优化思路中详细描述）</p>
<p>无疑YOLO v1的设计现在看来是比较简陋的，它有如下问题：</p>
<ul>
<li>最后输出的不同结果 对应原图的感受野都是相同的，这样对于不同尺度物体的匹配程度较差</li>
<li>每一个格子只能预测1个物体（可以设置为多个物体，但v1作者只是设置了2个，由此看来更多可能有一些问题）</li>
<li>YOLO v1是直接 对于坐标 的预测，而结果的感受野不是全图。（坐标肯定是要基于全图左上角作为基准才能得到）</li>
</ul>
<p>针对如上问题，有很多巧妙的方法被提出。这里简单介绍一下SSD和YOLO v2的anchor思想。具体参见优化思路。</p>
<p>SSD针对不同尺度的物体，使用最后五个特征图来预测。越深处的特征图的感受野越大，可以负责预测更大的物体，反之可以预测更小的物体。</p>
<p>YOLO v2则借用了Faster RCNN的anchor box思想。即预先设置好位置，然后通过预测该框的偏移来预测位置。（详见优化思路）</p>
<h2 id="优化思路"><a href="#优化思路" class="headerlink" title="优化思路"></a>优化思路</h2><h3 id="锚框（anchor-box）"><a href="#锚框（anchor-box）" class="headerlink" title="锚框（anchor  box）"></a>锚框（anchor  box）</h3><p>anchor box思想是Faster RCNN提出的（它改变了one-stage）。anchor box，顾名思义，锚框，像锚一些其基准的作用。</p>
<p>在anchor box提出之前，目标检测大多都是直接预测坐标的，这有很多缺点，比如不稳定、受到感受野限制等等。新的方法是首先在一张图片上铺满不同的锚框（主要是长宽比和大小不同）。</p>
<p>计算所有 anchor box 和 物体gt_box的 IoU。这样每一个anchor box的所负责预测类别便是与它IoU最大的那个gt_box。（附：当IoU小于某个阈值便作为负样本/背景样本）。</p>
<p>这样对物体的类别预测便是对anchor box类别的预测，对物体的位置的预测便是对anchor box位置的偏移的预测（对数偏移），其转换方法(YOLO v2)：</p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567323883396.png" alt="1567323883396"></p>
<p>然而锚框也有很大缺点：</p>
<ul>
<li>大量的锚框加剧了不平衡现象</li>
<li>锚框的长宽比需要手动设计</li>
<li>网络中预测物体的感受野 与 实际物体的 不匹配现象（位置不匹配）：在网络中，anchor box通过对数偏移预测实际物体位置，然而网络中的感受野没有变化。</li>
</ul>
<h3 id="多尺度"><a href="#多尺度" class="headerlink" title="多尺度"></a>多尺度</h3><p>影响目标检测性能的有两个问题：</p>
<ol>
<li>位置不匹配：物体的实际位置 和 对于感受野的不匹配</li>
<li>尺度不匹配：小感受野预测大物体 或 大感受野预测小物体</li>
</ol>
<p>所以很多方法被提出来解决尺度不匹配问题。如下图所示：</p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567324588539.png" alt="1567324588539"></p>
<ul>
<li>（a）是图片金子塔，将图片放缩成不同尺度来预测不同大小的物体，然而它有很多问题</li>
<li>（b）是老的目标检测网络用的backbone，只用最后一层来预测。尺度不匹配问题很严重</li>
<li>（c）是SSD提出的网络模型，使用最后几层不同尺度的特征图来预测物体，理论上小特征图的感受野大可以预测大物体，大特征图的感受野小，可以预测小物体。然而它有一个问题就是上层特征图可能对于信息的提取不充分。</li>
<li>（d）是特征金子塔，将深层特征图（语义信息充分，但缺乏空间信息）和浅层特征（空间信息充分，缺乏语义信息）融合。并将融合后的多尺度特征图预测物体。</li>
</ul>
<p>特征金子塔对于尺度不匹配问题有很好的解决。</p>
<h3 id="不平衡"><a href="#不平衡" class="headerlink" title="不平衡"></a>不平衡</h3><p>在目标检测中，样本主要分为入下几类：</p>
<ul>
<li>正/负 样本：正样本就是目标物体，负样本即背景物体</li>
<li>难/易 分样本：易分样本就是容易正确分类的样本，难分样本就是难以正确分类的样本</li>
</ul>
<p>不平衡问题也分为两类：</p>
<ul>
<li>正负样本不平衡：在训练集中，负样本占总体的比例很高。所以损失函数由负样本主导，不能很好对正样本进行优化。</li>
<li>难易样本不平衡：在训练集中，易分类样本远大于难分类样本，这些简单样本对损失函数和梯度的贡献很大，使得在迭代时不能够很好的优化。</li>
</ul>
<p>解决不平衡由很多方法，类似OHEM等，这里主要介绍何凯明提出的Focal loss：</p>
<p>传统的交叉熵的公式如下（二分类）：<br>$$<br>CE(p,y) =<br>\left{<br>\begin{array}{}<br>-log(p) \space\space\space\space\space\space\space\space\space\space\space\space if\space y = 1 \<br>-log(1-p) \space\space\space\space\space otherwise<br>\end{array}<br>\right.<br>$$<br>Focal loss的公式如下：<br>$$<br>FL(p)=<br>\left{<br>\begin{array}{}<br>-\alpha(1-p)^\gamma log(p)\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space if\space y = 1\<br>-(1-\alpha)(1-(1-p))^\gamma log(1-p)\space\space\space\space otherwise<br>\end{array}<br>\right.<br>$$<br>这里简化一下得到：<br>$$<br>\left{\begin{array}{}<br>FL(p_t) = -\alpha_t(1-p_t)^\gamma log(p_t)\<br>p_t = (y == 1\space?\space p\space:\space1-p)\<br>\alpha_t = (y == 1\space?\space \alpha\space:\space1-\alpha)\<br>\end{array}\right.<br>$$</p>
<p>相比于较叉熵，Focal loss加了两个参数：</p>
<ul>
<li>$\alpha$：$\alpha$是用来解决正负样本不平衡，它使得负样本的权重更小，正样本的权重更大</li>
<li>$(1-p_t)^\gamma$：$(1-p_t)^\gamma$是用来解决难易样本不平衡，$\gamma$是调制系数（一般$\gamma=2$）。当$1-p_t$ 越接近0，即代表该样本被正确分类，此时样本的权重越小。当$1-p_t$越小，说明该样本分类犯得错误越大，此时权重相对来说减小的更少，相当于变相的增加了权重。（$p_t = 0.99$损失小100倍，而$p_t = 0.5$仅缩小4倍）。</li>
</ul>
<h3 id="非极大值抑制（NMS）"><a href="#非极大值抑制（NMS）" class="headerlink" title="非极大值抑制（NMS）"></a>非极大值抑制（NMS）</h3><p>对于一个物体，神经网络可能会预测多个框，这些框围绕在gt_box附近。非极大值抑制就是为了抑制那些冗余的框，它的流程如下：</p>
<ul>
<li>将所有框的得分排序，选中最高分对应的框</li>
<li>遍历其余的框，如果和当前最高分框的重叠面积(IOU)大于一定阈值，我们就将框删除</li>
<li>从未处理的框中继续选一个得分最高的，重复上述过程，直到最后一个框。</li>
</ul>
<h2 id="经典模型"><a href="#经典模型" class="headerlink" title="经典模型"></a>经典模型</h2><h3 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h3><p>YOLO v3的网络结构如下：</p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567501439402.png" alt="1567501439402"></p>
<p>其中DBL就是基本卷积结构（conv+BN+leaky），resn是一个残差结构，concat是一个拼接操作。</p>
<p>其中YOLO v3是多尺度的输出，有y1、y2、y3三个输出，如此可以有效缓解尺度不匹配的现象。</p>
<p>YOLO v3借用了anchor box思想。每个anchor box预测输出为【x,y,w,h,o】.</p>
<p>其损失函数是二元分类的交叉熵。</p>
<h3 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h3><p>Faster RCNN的网络结构如下：</p>
<p><img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0%5C1567499632449.png" alt="1567499632449"></p>
<p>其网络主要分为三个部分：</p>
<ul>
<li>backbone：输入图片提取特征图</li>
<li>RPN：输入特征到RPN网络，对anchor进行二分类，其中正类就是有物体的，负类就是不含物体的。除了分类外，还对anchor进行第一次的位置回归，利用对数偏移调整anchor的位置。其中位置的损失函数是smooth L1，分类损失是交叉熵。</li>
<li>分类网络：将anchor按照RPN 网络的得分进行排序，剔除边界anchor、长宽过小的anchor，并进行NMS得到ROI。再通过ROI pooling输入分类回归网络，对ROI进行分类和第二次回归。</li>
</ul>
<p>smooth L1 loss的定义如下：<br>$$<br>smooth_{L1} =<br>\left{\begin{array}{}<br>0.5x^2\space\space\space\space\space\space\space\space if|x| &lt; 1\<br>|x|-0.5\space\space\space otherwise<br>\end{array}\right.<br>$$<br>其中x是预测框于gt_box之间的损失，其梯度为：<br>$$<br>smooth_{L1} =<br>\left{\begin{array}{}<br>x\space\space\space\space\space\space\space\space if|x| &lt; 1\<br>\pm1 \space\space\space\space\space otherwise<br>\end{array}\right.<br>$$<br>其好处是限制了梯度：</p>
<ul>
<li>当预测框与 ground truth 差别过大时，截断了梯度，使得梯度值不至于过大</li>
<li>当预测框与 ground truth 差别很小时，梯度值足够小</li>
</ul>
<h2 id="一些新的趋势（2018-2019）"><a href="#一些新的趋势（2018-2019）" class="headerlink" title="一些新的趋势（2018~2019）"></a>一些新的趋势（2018~2019）</h2><h3 id="anchor-free"><a href="#anchor-free" class="headerlink" title="anchor free"></a>anchor free</h3><p>技术的发展总是螺旋的。2017年，Faster RCNN提出的anchor机制以燎原之势占领了所有的目标检测的模型。而如今anchor free又成了新的研究热点。</p>
<p>anchor free的模型主要是通过特征点来确定预测box的。在corner-net中，作者提出通过检测一个物体的左上角和右下角来达到确定位置的效果。还有一些其他的例如预测中心点、四周点等等。这些预测特征点的方法和姿势检测，分割等领域相结合，成为了顶会的新宠儿。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>Recent Advances in Deep Learning for Object Detection</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/30/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/" data-id="ck21ppygr000jk4qnfg6h9c28" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/09/30/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E7%BB%93%E6%9E%84/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          卷积神经网络及其结构
        
      </div>
    </a>
  
  
    <a href="/2018/09/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">神经网络与优化器</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/">数据结构与算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E8%B0%88/">杂谈</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/cv/" rel="tag">cv</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dp/" rel="tag">dp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode%E6%80%BB%E7%BB%93/" rel="tag">leetcode总结</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" rel="tag">环境配置</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/cv/" style="font-size: 10px;">cv</a> <a href="/tags/dp/" style="font-size: 20px;">dp</a> <a href="/tags/leetcode%E6%80%BB%E7%BB%93/" style="font-size: 15px;">leetcode总结</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" style="font-size: 10px;">环境配置</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">神经网络</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">六月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/10/22/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2019/06/23/0-1%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/">0-1背包问题</a>
          </li>
        
          <li>
            <a href="/2018/10/20/ubuntu18-04-cuda-cudnn-Anaconda-%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E9%9A%9C%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB/">ubuntu18.04----cuda,cudnn,Anaconda,一个人工智障的自我修养</a>
          </li>
        
          <li>
            <a href="/2018/10/19/ubuntu18-04-%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85%E4%B8%8E%E7%BE%8E%E5%8C%96/">ubuntu18.04----基本配置与美化</a>
          </li>
        
          <li>
            <a href="/2018/09/30/Batch%20Normalization/">Batch Normlization</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Autumn-Cat<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>